{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to mallet \n",
    "\n",
    "os.environ[\"mallet_home\"] = r\"C:\\mallet\"\n",
    "path_to_mallet = r\"C:\\mallet\\bin\\mallet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cleaned dataset\n",
    "\n",
    "csv = \"C:/Users/laram/Downloads/cleaned_wiredfull.csv\"\n",
    "csv2 = \"C:/Users/laram/Downloads/finaledit.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_csv(csv2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['text_content'] = df_first['text_content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['text_content'] = clean_df['text_content'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further pre-processing with mallet \n",
    "\n",
    "training_data = [little_mallet_wrapper.process_string(text, numbers='remove') for text in clean_df['text_content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_texts = [text for text in clean_df['text_content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_titles = [title for title in clean_df['title']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in the tag column\n",
    "\n",
    "clean_df['tag'] = clean_df['tag'].fillna('Unlabelled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tags = [tag for tag in clean_df['tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "little_mallet_wrapper.print_dataset_stats(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics K\n",
    "\n",
    "num_topics = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set output directory path\n",
    "output_directory_path = \"C:/Users/laram/Downloads/ldamodel\"\n",
    "\n",
    "Path(f\"{output_directory_path}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_to_training_data           = f\"{output_directory_path}/training.txt\"\n",
    "path_to_formatted_training_data = f\"{output_directory_path}/mallet.training\"\n",
    "path_to_model                   = f\"{output_directory_path}/mallet.model.{str(num_topics)}\"\n",
    "path_to_topic_keys              = f\"{output_directory_path}/mallet.topic_keys.{str(num_topics)}\"\n",
    "path_to_topic_distributions     = f\"{output_directory_path}/mallet.topic_distributions.{str(num_topics)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "\n",
    "little_mallet_wrapper.quick_train_topic_model(path_to_mallet,\n",
    "                                             output_directory_path,\n",
    "                                             num_topics,\n",
    "                                             training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic keys \n",
    "\n",
    "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n",
    "\n",
    "data = [{\"Topic Number\": topic_number, \"Topic Keys\": topic} for topic_number, topic in enumerate(topics)]\n",
    "topicdf = pd.DataFrame(data)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(topicdf.head(40))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_path = \"C:/Users/laram/Downloads/topickeys_40lda.csv\"\n",
    "topicdf.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic distrubutions for each document \n",
    "\n",
    "topic_distributions = little_mallet_wrapper.load_topic_distributions(path_to_topic_distributions)\n",
    "\n",
    "topic_distributions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the topic with the highest proportion for each document\n",
    "\n",
    "dominant_topics = []\n",
    "for doc_id, distribution in enumerate(topic_distributions):\n",
    "    dominant_topic = max(enumerate(distribution), key=lambda x: x[1])[0]\n",
    "    dominant_proportion = max(distribution)\n",
    "    dominant_topics.append({\"Document ID\": doc_id, \"Dominant Topic\": dominant_topic, \"Proportion\": dominant_proportion})\n",
    "\n",
    "# Create a DataFrame from the list of results\n",
    "df_dominant_topics = pd.DataFrame(dominant_topics)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_dominant_topics)\n",
    "df_dominant_topics.to_csv(\"C:/Users/laram/Downloads/dominant_topics_40lda.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_clean_titles = dict(zip(training_data, clean_titles))\n",
    "training_data_original_text = dict(zip(training_data, original_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topic word probabilities \n",
    "\n",
    "topic_word_probability_dict = little_mallet_wrapper.load_topic_word_distributions(\"C:/Users/laram/Downloads/ldamodel/mallet.word_weights.25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 5 terms with the highest probability for each topic \n",
    "\n",
    "for _topic, _word_probability_dict in topic_word_probability_dict.items():\n",
    "    print('Topic', _topic)\n",
    "    for _word, _probability in sorted(_word_probability_dict.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(round(_probability, 4), '\\t', _word)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds with word proability list \n",
    "\n",
    "def create_word_cloud(topic_word_probabilities, topic_id,save_directory):\n",
    "    # Extract the word probabilities for the given topic_id\n",
    "    if topic_id not in topic_word_probabilities:\n",
    "        print(f\"Topic {topic_id} does not exist.\")\n",
    "        return\n",
    "\n",
    "    word_probabilities = topic_word_probabilities[topic_id]\n",
    "\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "\n",
    "    # Generate word cloud using the word probabilities\n",
    "    wordcloud.generate_from_frequencies(word_probabilities)\n",
    "    \n",
    "    # Sort word probabilities to get the words with frequencies\n",
    "    sorted_word_probabilities = sorted(word_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_20_words_with_frequencies = sorted_word_probabilities[:20]\n",
    "\n",
    "    # Create the subplot layout\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "    # Plot the word cloud\n",
    "    ax[0].imshow(wordcloud, interpolation='bilinear')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title(f'Word Cloud for Topic {topic_id}')\n",
    "\n",
    "    # Display the top 20 words with frequencies\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Top 20 Words with Frequencies')\n",
    "    words_with_freq_text = \"\\n\".join([f\"{word}: {freq:.4f}\" for word, freq in top_20_words_with_frequencies])\n",
    "    ax[1].text(0.5, 0.5, words_with_freq_text, horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = f\"{save_directory}/cloud_topic_{topic_id}.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Word cloud for Topic {topic_id} saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloud for topic 6\n",
    "\n",
    "topic_id = 6 \n",
    "save_directory = \"C:/Users/laram/Downloads\"  \n",
    "create_word_cloud(topic_word_probability_dict, topic_id, save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from the diagnostic files such as effective number of words score and u_mass coherence\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Specify the number of topics you're interested in\n",
    "num_topics = 25 # Example: Choose the number of topics you want to analyze\n",
    "\n",
    "# Path to your diagnostics directory and the specific diagnostic file for the chosen number of topics\n",
    "diagnostics_dir = \"C:/Users/laram/Downloads/cohmodel\"\n",
    "diagnostic_file = os.path.join(diagnostics_dir, f\"diagnostics_{num_topics}.xml\")\n",
    "\n",
    "# Check if the diagnostic file exists\n",
    "if not os.path.exists(diagnostic_file):\n",
    "    print(f\"Diagnostic file for {num_topics} topics not found.\")\n",
    "else:\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(diagnostic_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Extract scores\n",
    "    topic_scores = []\n",
    "    for topic in root.findall('.//topic'):\n",
    "        topic_number = int(topic.get('id'))\n",
    "        score = float(topic.get('eff_num_words'))\n",
    "        topic_scores.append((topic_number, score))\n",
    "    \n",
    "    # Unzip topic numbers and scores\n",
    "    topics, scores = zip(*topic_scores)\n",
    "    \n",
    "    # Plot scores \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(topics, scores, s=scores, alpha=0.5, c=scores, cmap='viridis', edgecolors='k')\n",
    "    \n",
    "    # Annotate each circle with its topic number\n",
    "    for topic_number, score in zip(topics, scores):\n",
    "        plt.text(topic_number, score, str(topic_number), ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(scatter, label='Coherence Score')\n",
    "    plt.title(f'Effective Number of Words Score for {num_topics} Topics')\n",
    "    plt.xlabel('Topic Number')\n",
    "    plt.ylabel('Effective Number of Words Score')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path = \"C:/Users/laram/Downloads/eff_num.png\"\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = little_mallet_wrapper.load_topic_keys(path_to_topic_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top representative documents for each topic \n",
    "\n",
    "def display_top_topics(topic_number, topics, training_data, topic_distributions, training_data_clean_titles, number_of_documents=5):\n",
    "\n",
    "    # Fetch top documents for the specified topic number\n",
    "    top_documents = little_mallet_wrapper.get_top_docs(training_data, topic_distributions, topic_number, n=number_of_documents)\n",
    "    \n",
    "    # Print topic number and keys\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{' '.join(topics[topic_number])}\\n\")\n",
    "    \n",
    "    # Print top documents\n",
    "    for probability, document_index in top_documents:\n",
    "        document_title = training_data_clean_titles[document_index]\n",
    "        print(f\"Probability: {round(probability, 4)}\")\n",
    "        print(f\"Title: {document_title}\\n\")\n",
    "    \n",
    "    # Create a DataFrame for top documents\n",
    "    data = [{\n",
    "        \"Topic Number\": topic_number,\n",
    "        \"Topic Keys\": \", \".join(topics[topic_number]),\n",
    "        \"Probability\": round(probability, 4),\n",
    "        \"Document Title\": training_data_clean_titles[document_index]\n",
    "    } for probability, document_index in top_documents]\n",
    "    \n",
    "    top_docs_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Display and save DataFrame\n",
    "    print(top_docs_df.head(10))\n",
    "    top_docs_df.to_csv(f\"C:/Users/laram/Downloads/topic_top_documents{topic_number}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topic number for analysis \n",
    "\n",
    "topic_number_to_analyze = 25 \n",
    "display_top_topics(topic_number_to_analyze, topics, training_data, topic_distributions, training_data_clean_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_10 = topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame for results from K=10 model\n",
    "\n",
    "def summarize_topics_to_df(topics):\n",
    "    summaries = []\n",
    "    for topic_id, topic_words in enumerate(topics):\n",
    "        summaries.append({\n",
    "            \"Topic ID\": topic_id,\n",
    "            \"Keywords\": \", \".join(topic_words)\n",
    "        })\n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "summaries_df_10 = summarize_topics_to_df(topics_10)\n",
    "print(summaries_df_10)\n",
    "summaries_df_10.to_csv(f\"C:/Users/laram/Downloads/summaries_topics10.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=2, number_of_documents=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_top_titles_per_topic(topic_number=3, number_of_documents=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap with tag categories as target labels \n",
    "\n",
    "topic_keys_file = r\"C:\\Users\\laram\\Downloads\\ldamodel\\mallet.topic_keys.25\"\n",
    "topic_distributions_file = r\"C:\\Users\\laram\\Downloads\\ldamodel\\mallet.topic_distributions.25\"\n",
    "output_directory_path = r\"C:\\Users\\laram\\Downloads\"\n",
    "\n",
    "# Load the topic keys\n",
    "topics = little_mallet_wrapper.load_topic_keys(topic_keys_file)\n",
    "\n",
    "# Load the topic distributions\n",
    "topic_distributions = little_mallet_wrapper.load_topic_distributions(topic_distributions_file)\n",
    "\n",
    "print(len(topics))\n",
    "print(len(target_labels))\n",
    "print(len(topic_distributions))\n",
    "\n",
    "print(\"First entry in topic_distributions:\", topic_distributions[0])\n",
    "print(\"Length of topic_distributions:\", len(topic_distributions))\n",
    "\n",
    "\n",
    "target_labels = clean_tags\n",
    "\n",
    "print(\"First 5 entries in clean_titles:\", clean_titles[:5])\n",
    "print(\"First 5 entries in target_labels:\", target_labels[:5])\n",
    "\n",
    "try:\n",
    "    little_mallet_wrapper.plot_categories_by_topics_heatmap(\n",
    "        clean_tags,\n",
    "        topic_distributions,\n",
    "        topics,\n",
    "        output_directory_path + r'\\categories_by_topics.pdf',\n",
    "        target_labels=target_labels,\n",
    "        dim=(18, 8)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error during plotting:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys_file = r\"C:\\Users\\laram\\Downloads\\ldamodel\\mallet.topic_keys.40\"\n",
    "topic_distributions_file = r\"C:\\Users\\laram\\Downloads\\ldamodel\\mallet.topic_distributions.40\"\n",
    "output_path = r\"C:\\Users\\laram\\Downloads\\topics_over_time.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys = little_mallet_wrapper.load_topic_keys(topic_keys_file)\n",
    "\n",
    "# Load the topic distributions\n",
    "topic_distributions = little_mallet_wrapper.load_topic_distributions(topic_distributions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_years = clean_df['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topics over time for a single topic \n",
    "\n",
    "topic_index = 8  # Set topic number\n",
    "little_mallet_wrapper.plot_topics_over_time(topic_distributions, topic_keys, clean_years, topic_index, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index = 19\n",
    "output_path = r\"C:\\Users\\laram\\Downloads\\topics_over_time_topic_{}.png\".format(topic_index)\n",
    "\n",
    "\n",
    "little_mallet_wrapper.plot_topics_over_time(topic_distributions, topic_keys, clean_years, topic_index, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index = 35\n",
    "output_path = r\"C:\\Users\\laram\\Downloads\\topics_over_time_topic_{}.png\".format(topic_index)\n",
    "little_mallet_wrapper.plot_topics_over_time(topic_distributions, topic_keys, clean_years, topic_index, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the topic coherence scores \n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load and parse the diagnostics file\n",
    "file_path = \"C:/Users/laram\\Downloads/ldamodel/mallet.diagnostics.15.xml\"  # Update with the actual path to your diagnostics file\n",
    "tree = ET.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract topic metrics\n",
    "topics = root.findall('.//topic')\n",
    "\n",
    "# Print coherence for each topic\n",
    "for topic in topics:\n",
    "    topic_id = topic.get('id')\n",
    "    coherence = topic.get('coherence')\n",
    "    print(f'Topic {topic_id} Coherence: {coherence}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
