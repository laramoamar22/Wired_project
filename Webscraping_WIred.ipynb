{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Set url with all the archive links \n",
    "\n",
    "url= \"https://www.wired.com/sitemap/\"\n",
    "urlPages = [] #empty list to store archive links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download url page with links using BeautifulSoup\n",
    "\n",
    "page = requests.get(url, verify=False)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all links in the sitemap section\n",
    "\n",
    "urlList = soup.find(\"div\", class_=\"sitemap__section-archive\")\n",
    "#print(urlList)\n",
    "\n",
    "links = urlList.find_all('a')\n",
    "#print(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the links between 1990 and 2001\n",
    "\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href:\n",
    "        # Debugging: Print the href to check its format\n",
    "        print(f\"Checking href: {href}\")\n",
    "        \n",
    "        # Extract year from the href attribute\n",
    "        year_index = href.find(\"year=\")\n",
    "        if year_index != -1:\n",
    "            # Debugging: Print the year substring to verify extraction\n",
    "            year_str = href[year_index + 5: year_index + 9]\n",
    "            print(f\"Extracted year substring: {year_str}\")\n",
    "            \n",
    "            # Ensure the extracted substring is a digit\n",
    "            if year_str.isdigit():\n",
    "                year = int(year_str)\n",
    "                \n",
    "                # Check if the year falls within 1990 to 2001\n",
    "                if 1990 <= year <= 2001:\n",
    "                    full_link = \"https://www.wired.com\" + href\n",
    "                    urlPages.append(full_link)\n",
    "                    # Debugging: Print the full link being added\n",
    "                    print(f\"Added link: {full_link}\")\n",
    "\n",
    "\n",
    "#print(\"Final list of URLs:\")\n",
    "#print(urlPages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urlPages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract article links from each issue URL\n",
    "\n",
    "\n",
    "def extract_article_links(issue_url):\n",
    "    full_article_links = []\n",
    "    page = requests.get(issue_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Find the articles section\n",
    "    articles = soup.find(\"div\", class_=\"sitemap__section-archive\")\n",
    "    \n",
    "    if not articles:\n",
    "        # If no articles section is found, return a message or empty list\n",
    "        print(f\"No articles section found for URL: {issue_url}\")\n",
    "        return \"No articles section found\"\n",
    "    \n",
    "    # Find all article links within the section\n",
    "    article_links = articles.find_all('a')\n",
    "    \n",
    "    if not article_links:\n",
    "        # If no article links are found, return a message or empty list\n",
    "        print(f\"No articles found for URL: {issue_url}\")\n",
    "        return \"No articles found\"\n",
    "    \n",
    "    # Cycle through extracted links and appennd base URL\n",
    "    for link in article_links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            full_link = \"https://www.wired.com\" + href\n",
    "            full_article_links.append(full_link)\n",
    "    \n",
    "    return full_article_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set empty dictionary with years as keys with defined lists for issue urls and assoicated article links for each issue \n",
    "\n",
    "chosen_links_articles = {}\n",
    "\n",
    "# Cycle through all issue URLs \n",
    "for chosen_link in urlPages:\n",
    "    year_str = chosen_link.split(\"year=\")[-1]  # Split at \"year=\" and take the last part\n",
    "    year = year_str.split(\"&\")[0]  # Split at \"&\" to remove the rest of the URL parameters\n",
    "    article_links = extract_article_links(chosen_link)\n",
    "    if year not in chosen_links_articles:\n",
    "        chosen_links_articles[year] = {'issue_links': [], 'articles': []}  # Update Year as key \n",
    "    chosen_links_articles[year]['issue_links'].append(chosen_link)  # Add the issue link to the list for the year\n",
    "    chosen_links_articles[year]['articles'].extend(article_links)  # Add article links to the list for the year\n",
    "\n",
    "    \n",
    "for year, data in chosen_links_articles.items():\n",
    "    print(f\"Year: {year}\")\n",
    "    print(\"Issue URLs:\")\n",
    "    for issue_link in data['issue_links']:\n",
    "        print(issue_link)\n",
    "    print(\"Article URLs:\")\n",
    "    for article_link in data['articles']:\n",
    "        print(article_link)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url, retries=5, backoff_factor=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)  # Increase the timeout duration\n",
    "            response.raise_for_status()  \n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} for URL {url} failed: {e}\")\n",
    "            time.sleep(backoff_factor * (2 ** attempt))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract all the relevant information: Title, Year, Summary Description, Tag category, Article text content \n",
    "# from article URLs and return a dictionary \n",
    "\n",
    "def extract_article_info(article_url):\n",
    "    # Request the page and parse it with BeautifulSoup\n",
    "    page = fetch_url(article_url)\n",
    "    if page is None:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Find title\n",
    "    title_element = soup.find('h1', {'data-testid': 'ContentHeaderHed', 'class': 'BaseWrap-sc-gjQpdd BaseText-ewhhUZ ContentHeaderHed-NCyCC iUEiRd htVlUB kctZMs'})\n",
    "    title = title_element.text.strip() if title_element else None\n",
    "    \n",
    "    # Find year\n",
    "    time_element = soup.find('time', {'data-testid': 'ContentHeaderPublishDate', 'class': 'BaseWrap-sc-gjQpdd BaseText-ewhhUZ ContentHeaderTitleBlockPublishDate-hYmSqb iUEiRd jpVMoQ cXawal'})\n",
    "    datetime_value = time_element.get('datetime') if time_element else None\n",
    "    year = datetime_value[:4] if datetime_value else None\n",
    "    \n",
    "    # Find summary\n",
    "    summary_element = soup.find('div', class_='ContentHeaderDek-bIqFFZ fOichq')\n",
    "    summary = summary_element.text.strip() if summary_element else None\n",
    "    \n",
    "    # Find tag category \n",
    "    tag_element = soup.find('span', class_='RubricName-fVtemz cLxcNi rubric__name')\n",
    "    tag = tag_element.text.strip() if tag_element else None\n",
    "    \n",
    "    # Find text content\n",
    "    body_element = soup.find('div', class_='body__inner-container')\n",
    "    text_content = body_element.text.strip() if body_element else None\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    ''\n",
    "    return {\n",
    "        'title': title,\n",
    "        'year': year,\n",
    "        'summary': summary,\n",
    "        'tag': tag,\n",
    "        'text_content': text_content\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define function to extraact the article information from all the article URLs\n",
    "\n",
    "def extract_articles_info(article_links):\n",
    "    articles_info = []\n",
    "    for article_link in article_links:\n",
    "        time.sleep(1)  # Delay to avoid hitting the server too hard\n",
    "        article_info = extract_article_info(article_link)\n",
    "        if article_info:  # Check article_info is not None \n",
    "            articles_info.append(article_info)\n",
    "    return articles_info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define function to scrape and save article information into csv files in batches of 100 \n",
    "\n",
    "\n",
    "def scrape_and_save_articles(chosen_links_articles, batch_size=100):\n",
    "    for year, data in chosen_links_articles.items():\n",
    "        issue_links = data['issue_links']\n",
    "        articles_links = data['articles']\n",
    "        \n",
    "        total_articles = len(articles_links)\n",
    "        start_batch = 0\n",
    "\n",
    "        # Check for existing files to determine the next batch to process\n",
    "        while True:\n",
    "            batch_csv_filename = f'articles_{year}_batch_{start_batch + 1}.csv'\n",
    "            if os.path.exists(batch_csv_filename):\n",
    "                start_batch += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        for i in range(start_batch * batch_size, total_articles, batch_size):\n",
    "            batch_articles_links = articles_links[i:i + batch_size]  # Get the next batch of articles\n",
    "            batch_articles_info = extract_articles_info(batch_articles_links)  # Extract article information\n",
    "            \n",
    "            # Convert the list of dictionaries to a DataFrame\n",
    "            batch_articles_df = pd.DataFrame(batch_articles_info)\n",
    "            \n",
    "            # Save to a CSV file for the current batch\n",
    "            batch_csv_filename = f'articles_{year}_batch_{i // batch_size + 1}.csv'\n",
    "            batch_articles_df.to_csv(batch_csv_filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            print(f\"Articles for year {year}, batch {i // batch_size + 1} saved to {batch_csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_and_save_articles_in_chunks(chosen_links_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Define function to merge the CSV batches for each year into a single file \n",
    "\n",
    "def merge_batch_csvs_to_yearly_csvs(chosen_links_articles):\n",
    "    for year in chosen_links_articles.keys():\n",
    "        all_files = glob.glob(f'articles_{year}_batch_*.csv')  # Get all batch files for the specified year\n",
    "        all_df = []\n",
    "\n",
    "        for file in all_files:\n",
    "            df = pd.read_csv(file)\n",
    "            all_df.append(df)\n",
    "\n",
    "        merged_df = pd.concat(all_df, ignore_index=True)\n",
    "        yearly_csv_filename = f'articles_{year}.csv'\n",
    "        merged_df.to_csv(yearly_csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"All batches for year {year} merged into {yearly_csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_batch_csvs_to_yearly_csvs(chosen_links_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to merge all yearly CSV files into a single file with full dataset \n",
    "\n",
    "def merge_all_yearly_csvs(chosen_links_articles):\n",
    "    all_yearly_files = [f'articles_{year}.csv' for year in chosen_links_articles.keys() if os.path.exists(f'articles_{year}.csv')]\n",
    "    if not all_yearly_files:\n",
    "        print(\"No yearly files found to merge.\")\n",
    "        return\n",
    "\n",
    "    all_df = []\n",
    "\n",
    "    for file in all_yearly_files:\n",
    "        df = pd.read_csv(file)\n",
    "        all_df.append(df)\n",
    "\n",
    "    final_merged_df = pd.concat(all_df, ignore_index=True)\n",
    "    final_csv_filename = 'all_articles_merged.csv'\n",
    "    final_merged_df.to_csv(final_csv_filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"All yearly CSV files merged into {final_csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_all_yearly_csvs(chosen_links_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
