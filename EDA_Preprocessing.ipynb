{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset into Pandas dataframe \n",
    "df = pd.read_csv('all_articles_merged.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first five rows \n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tag categories \n",
    "tag_column = df['tag']\n",
    "print(tag_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for unique tags\n",
    "unique_tags = df['tag'].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(\"Unique tags:\", unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows using len\n",
    "num_rows = len(df)\n",
    "\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of null values for text_content column which contains article texts\n",
    "\n",
    "null_count = df['text_content'].isnull().sum()\n",
    "print(\"Number of null values in 'textcontent' column:\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the rows with null values in 'textcontent' column\n",
    "\n",
    "null_rows = df[df['text_content'].isnull()]\n",
    "print(\"Rows with null values in 'textcontent' column:\")\n",
    "print(null_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values for text_content column \n",
    "df_null= df.dropna(subset=['text_content'])\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(\"DataFrame after dropping rows with null values in 'textcontent' column:\")\n",
    "print(df_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates \n",
    "\n",
    "duplicates = df_null.duplicated()\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates \n",
    "df_unique = df_null.drop_duplicates()\n",
    "num_rows = len(df_unique)\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_unique.dropna(subset=['text_content'])\n",
    "df_original = df_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframe to csv file \n",
    "\n",
    "df_cleaned.to_csv('cleaned_wiredfull.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(df_cleaned)\n",
    "print(\"Number of rows:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for the 'year' column to show yearly distibution of artices \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['year'], bins=range(df_cleaned['year'].min(), df_cleaned['year'].max() + 2), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Yearly Distribution of Articles')\n",
    "plt.xticks(range(df_cleaned['year'].min(), df_cleaned['year'].max() + 1))\n",
    "plt.grid(True)\n",
    "plt.savefig('histogram_articles_per_year.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_per_year = df_cleaned['year'].value_counts().sort_index()\n",
    "print(\"Number of articles per year:\")\n",
    "print(articles_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('histogram_articles_per_year.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    # Split text into words and count them\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Apply the function to each row in 'textcontent' column\n",
    "df_cleaned['word_count'] = df_cleaned['text_content'].apply(count_words)\n",
    "\n",
    "# Calculate mean article length\n",
    "mean_article_length = df_cleaned['word_count'].mean()\n",
    "print(\"\\nMean article length:\", mean_article_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = df_cleaned['tag'].isnull().sum()\n",
    "print(\"Number of null values in 'tag' column:\", null_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null tag values as Unlabelled \n",
    "df_pie = df_cleaned.copy()\n",
    "df_pie['tag'] = df_pie['tag'].fillna('Unlabelled')\n",
    "print(df_pie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = df_pie['tag'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot for each tag\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(data=df_pie, x='year', hue='tag')\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Distribution of Tags Across Years')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Tag')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('tags_distribution_across_years.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group for top tags per year \n",
    "\n",
    "top_tags_per_year = df_pie.groupby(['year', 'tag']).size().reset_index(name='count')\n",
    "top_tags_per_year = top_tags_per_year.sort_values(['year', 'count'], ascending=[True, False])\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=top_tags_per_year, x='year', y='count', hue='tag', dodge=False)\n",
    "plt.title('Top Tags by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Tag')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('top_tags_across_years.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tags_per_year = df_pie.groupby(['year', 'tag']).size().reset_index(name='count')\n",
    "\n",
    "# Define the number of top tags to display per year\n",
    "top_n = 5\n",
    "\n",
    "# Get the top N tags per year\n",
    "top_tags_per_year = top_tags_per_year.sort_values(['year', 'count'], ascending=[True, False])\n",
    "top_tags_per_year = top_tags_per_year.groupby('year').head(top_n).reset_index(drop=True)\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(data=top_tags_per_year, x='year', y='count', hue='tag', dodge=False)\n",
    "\n",
    "# Set plot title and labels with larger font size\n",
    "plt.title('Top Tags by Year', fontsize=20)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(title='Tag', fontsize=14, title_fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_tags_by_year2.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packaged for pre-processing \n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk import word_tokenize #Import word_tokenize function from NLTK\n",
    "from nltk.corpus import stopwords #Import the stop words lists from NLTK\n",
    "import string  # Import the string module\n",
    "from string import punctuation \n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "new= df_cleaned['title'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    if word in stop_words:\n",
    "        dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_stopwords_barchart(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "    from collections import defaultdict\n",
    "    dic=defaultdict(int)\n",
    "    for word in corpus:\n",
    "        if word in stop_words:\n",
    "            dic[word]+=1\n",
    "            \n",
    "    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    x,y=zip(*top)\n",
    "    plt.bar(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_stopwords_barchart(df_cleaned['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "\n",
    "x, y= [], []\n",
    "\n",
    "for word,count in most[:40]:\n",
    "    if (word not in stop_words):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_non_stopwords_barchart(text):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_nopunct = text.apply(lambda x: x.translate(translator))\n",
    "    \n",
    "    new = text_nopunct.str.split()\n",
    "    new = new.values.tolist()\n",
    "    corpus = [word for i in new for word in i]\n",
    "\n",
    "    # Filter out stopwords from corpus\n",
    "    corpus = [word.lower() for word in corpus if word.lower() not in stop]\n",
    "\n",
    "    # Count frequencies of non-stopwords\n",
    "    counter = Counter(corpus)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    top_words = [word for word, count in most[:20]]  # Top 20 non-stopwords\n",
    "\n",
    "\n",
    "    sns.barplot(x=[count for word, count in most[:20]], y=top_words, palette='viridis')\n",
    "    sns.set(rc={'figure.figsize':(10, 6)})  # Adjust figure size if necessary\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title('Top 20 words in Wired Headlines')\n",
    "    \n",
    "    plt.savefig('top_words_barplot.png', dpi=300, bbox_inches='tight')  # Adjust filename and parameters as needed\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_titles_with_top_words(df, top_words):\n",
    "    print(\"Titles containing top words:\")\n",
    "    top_words_set = set(word.lower() for word in top_words)\n",
    "    \n",
    "    for title in df_cleaned['title']:\n",
    "        title_words = set(title.lower().split())\n",
    "        if top_words_set & title_words:\n",
    "            print(title)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = plot_top_non_stopwords_barchart(df_cleaned['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print titles containing top words\n",
    "print_titles_with_top_words(df_cleaned, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_non_stopwords_barchart(df_cleaned['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append 'said' and 'say' to stopword list because they have a high frequency \n",
    "\n",
    "stop_words.append('said')\n",
    "stop_words.append('say')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing text data for LDA pipeline \n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(\"<[^>]+>|\\[.*?\\]|div|br|span\", ' ', text) #remove html tags \n",
    "    text = re.sub(r'<a\\s+href=\".*?\">(.*?)</a>', r'\\1', text) #remove links \n",
    "    text = re.sub(r\"\\.\\.\\.\", \" \", text)\n",
    "    text = re.sub(r'\\b\\d+\\b', \" \", text) #remove digits \n",
    "    text = re.sub(r'(\\w+)-(\\w+)', r'\\1\\2', text) #remove hyphen and join words\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    text = re.sub(r'\\bus\\b', 'us', text)\n",
    "\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation and digits\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove tokens that contain non-alphabetic characters\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += ['say']\n",
    "    stop_words += ['said']\n",
    "\n",
    "\n",
    "\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatisation \n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['text_content'] = df_cleaned['text_content'].apply(preprocess_text)\n",
    "#df_original['text_content'] = df_original['text_content'].apply(preprocess_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check for null values to ensure none are found \n",
    "\n",
    "null_values = df_cleaned['text_content'].isnull().sum()\n",
    "\n",
    "if null_values > 0:\n",
    "    print(f\"There are {null_values} null values in the cleaned text column.\")\n",
    "else:\n",
    "    print(\"No null values found in the cleaned text column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_text = ' '.join(df_cleaned['text_content'])\n",
    "#all_processed_text = ' '.join(df_original['text_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file \n",
    "df_cleaned[\"text_content\"].to_csv('tryclean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(all_processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(all_processed_text.split())\n",
    "print(corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(set(all_processed_text.split()))\n",
    "print(f\"Vocabulary size (number of unique tokens): {vocabulary_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of text length (via token count) of original and processed data \n",
    "\n",
    "original_lengths = [len(text.split()) for text in df_zero['text_content']]\n",
    "processed_lengths = [len(text.split()) for text in df_cleaned['text_content']]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.hist(original_lengths, bins=30, alpha=0.5, label='Original Text')\n",
    "plt.hist(processed_lengths, bins=30, alpha=0.5, label='Processed Text')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Text Length Comparison')\n",
    "plt.legend()\n",
    "plt.savefig('processingcomparison.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
